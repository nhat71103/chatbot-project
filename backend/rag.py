import re
from typing import List, Tuple, Optional
import requests
from bs4 import BeautifulSoup
import urllib.parse

from sqlalchemy.orm import Session

from db import Knowledge, get_session


def tokenize(text: str) -> List[str]:
    """TÃ¡ch tá»« Ä‘Æ¡n giáº£n, bá» kÃ½ tá»± Ä‘áº·c biá»‡t vÃ  chuyá»ƒn vá» lowercase."""
    text = text.lower()
    # TÃ¡ch theo chá»¯ cÃ¡i vÃ  sá»‘, bá» dáº¥u cÃ¢u
    return re.findall(r"[a-zA-Z0-9_Ã€-á»¹]+", text)


def expand_keywords(tokens: List[str]) -> List[str]:
    """
    Má»Ÿ rá»™ng tá»« khÃ³a vá»›i cÃ¡c tá»« Ä‘á»“ng nghÄ©a/liÃªn quan Ä‘á»ƒ tÃ¬m kiáº¿m linh hoáº¡t hÆ¡n.
    """
    synonyms = {
        "giá»i": ["tá»‘t", "xuáº¥t sáº¯c", "thÃ nh tháº¡o", "giá»i giang"],
        "lá»£i tháº¿": ["Æ°u Ä‘iá»ƒm", "cÃ³ Ã­ch", "há»¯u Ã­ch", "tá»‘t", "cÃ³ lá»£i"],
        "áº£nh hÆ°á»Ÿng": ["tÃ¡c Ä‘á»™ng", "áº£nh hÆ°á»Ÿng", "liÃªn quan"],
        "cáº§n thiáº¿t": ["quan trá»ng", "cáº§n", "cáº§n cÃ³", "cáº§n dÃ¹ng"],
        "khÃ´ng": ["khÃ´ng", "chÆ°a", "thiáº¿u"],
        "cÃ³": ["cÃ³", "sá»Ÿ há»¯u", "Ä‘Æ°á»£c"],
        "há»c": ["há»c", "nghiÃªn cá»©u", "tÃ¬m hiá»ƒu"],
        "toÃ¡n": ["toÃ¡n há»c", "toÃ¡n", "math"],
        "cntt": ["cÃ´ng nghá»‡ thÃ´ng tin", "cntt", "it", "tin há»c"],
    }
    
    expanded = set(tokens)
    for token in tokens:
        if token in synonyms:
            expanded.update(synonyms[token])
    return list(expanded)


def score_text(query_tokens: List[str], text: str) -> int:
    """
    TÃ­nh Ä‘iá»ƒm similarity cáº£i thiá»‡n giá»¯a query vÃ  má»™t Ä‘oáº¡n text:
    - Má»Ÿ rá»™ng tá»« khÃ³a vá»›i tá»« Ä‘á»“ng nghÄ©a
    - +3 Ä‘iá»ƒm náº¿u tá»« trÃ¹ng chÃ­nh xÃ¡c
    - +2 Ä‘iá»ƒm náº¿u tá»« Ä‘á»“ng nghÄ©a trÃ¹ng
    - +1 Ä‘iá»ƒm náº¿u tá»« con náº±m trong tá»« lá»›n hÆ¡n
    Bá» qua stopwords ráº¥t phá»• biáº¿n.
    """
    if not text:
        return 0

    stopwords = {"lÃ ", "vÃ ", "cÃ¡c", "má»™t", "nhá»¯ng", "khi", "Ä‘á»ƒ", "trong", "vá»›i", "thÃ¬", "cÃ³", "khÃ´ng", "gÃ¬"}
    text_lower = text.lower()
    score = 0
    
    # Má»Ÿ rá»™ng tá»« khÃ³a vá»›i tá»« Ä‘á»“ng nghÄ©a
    expanded_tokens = expand_keywords(query_tokens)

    for w in expanded_tokens:
        if w in stopwords and w not in query_tokens:  # Chá»‰ bá» stopwords khÃ´ng cÃ³ trong query gá»‘c
            continue
        
        # Khá»›p chÃ­nh xÃ¡c theo tá»«
        if re.search(rf"\b{re.escape(w)}\b", text_lower):
            # Tá»« gá»‘c trong query Ä‘Æ°á»£c Ä‘iá»ƒm cao hÆ¡n
            if w in query_tokens:
                score += 3
            else:
                score += 2  # Tá»« Ä‘á»“ng nghÄ©a
        elif w in text_lower:
            score += 1

    return score


def search_wikipedia(query: str, lang: str = "vi") -> Optional[str]:
    """
    TÃ¬m kiáº¿m thÃ´ng tin tá»« Wikipedia API.
    Tráº£ vá» Ä‘oáº¡n text Ä‘áº§u tiÃªn cá»§a bÃ i viáº¿t phÃ¹ há»£p nháº¥t.
    """
    try:
        # URL encode query
        query_encoded = urllib.parse.quote(query.replace(" ", "_"))
        
        # Thá»­ tÃ¬m kiáº¿m bÃ i viáº¿t trá»±c tiáº¿p
        search_url = f"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{query_encoded}"
        response = requests.get(search_url, timeout=5, headers={"User-Agent": "Chatbot/1.0"})
        
        if response.status_code == 200:
            data = response.json()
            extract = data.get("extract", "")
            if extract:
                # Giá»›i háº¡n Ä‘á»™ dÃ i Ä‘á»ƒ khÃ´ng quÃ¡ dÃ i
                if len(extract) > 600:
                    extract = extract[:600] + "..."
                return extract
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y, thá»­ tÃ¬m kiáº¿m báº±ng API search
        search_api_url = f"https://{lang}.wikipedia.org/api/rest_v1/page/search/{urllib.parse.quote(query)}"
        response = requests.get(search_api_url, params={"limit": 1}, timeout=5, headers={"User-Agent": "Chatbot/1.0"})
        
        if response.status_code == 200:
            results = response.json()
            pages = results.get("pages", [])
            if pages:
                page_title = pages[0].get("title", "")
                if page_title:
                    # Thá»­ láº¡i vá»›i title chÃ­nh xÃ¡c
                    title_encoded = urllib.parse.quote(page_title.replace(" ", "_"))
                    summary_url = f"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{title_encoded}"
                    response2 = requests.get(summary_url, timeout=5, headers={"User-Agent": "Chatbot/1.0"})
                    if response2.status_code == 200:
                        data = response2.json()
                        extract = data.get("extract", "")
                        if extract:
                            if len(extract) > 600:
                                extract = extract[:600] + "..."
                            return extract
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y tiáº¿ng Viá»‡t, thá»­ tiáº¿ng Anh
        if lang == "vi":
            return search_wikipedia(query, lang="en")
        
        return None
    except Exception as e:
        # Náº¿u cÃ³ lá»—i, thá»­ tiáº¿ng Anh náº¿u Ä‘ang á»Ÿ tiáº¿ng Viá»‡t
        if lang == "vi":
            try:
                return search_wikipedia(query, lang="en")
            except:
                return None
        return None


def search_web_simple(query: str) -> Optional[str]:
    """
    TÃ¬m kiáº¿m Ä‘Æ¡n giáº£n trÃªn web báº±ng cÃ¡ch tÃ¬m kiáº¿m tá»« khÃ³a chÃ­nh.
    Tráº£ vá» Ä‘oáº¡n text ngáº¯n tá»« káº¿t quáº£ tÃ¬m kiáº¿m.
    """
    try:
        # Láº¥y tá»« khÃ³a chÃ­nh (bá» stopwords, Æ°u tiÃªn tá»« dÃ i hÆ¡n)
        tokens = tokenize(query)
        stopwords = {"lÃ ", "vÃ ", "cÃ¡c", "má»™t", "nhá»¯ng", "khi", "Ä‘á»ƒ", "trong", "vá»›i", "thÃ¬", "cÃ³", "khÃ´ng", "gÃ¬", "khÃ´ng"}
        keywords = [t for t in tokens if t not in stopwords and len(t) > 2]
        
        # Sáº¯p xáº¿p theo Ä‘á»™ dÃ i (tá»« dÃ i hÆ¡n = cá»¥ thá»ƒ hÆ¡n)
        keywords.sort(key=len, reverse=True)
        
        if not keywords:
            return None
        
        # Thá»­ tÃ¬m trÃªn Wikipedia vá»›i tá»« khÃ³a Ä‘áº§u tiÃªn (quan trá»ng nháº¥t)
        # VÃ­ dá»¥: "python lÃ  gÃ¬" -> tÃ¬m "python"
        main_keyword = keywords[0]
        result = search_wikipedia(main_keyword)
        if result:
            return result
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y, thá»­ vá»›i toÃ n bá»™ cÃ¢u há»i (loáº¡i bá» dáº¥u cÃ¢u)
        query_clean = re.sub(r'[?.,!]', '', query).strip()
        if query_clean and query_clean != main_keyword:
            result = search_wikipedia(query_clean)
            if result:
                return result
        
        return None
    except Exception:
        return None


class RAGChatbot:
    """
    Chatbot RAG vá»›i kháº£ nÄƒng tÃ¬m kiáº¿m web:
    - Æ¯u tiÃªn tÃ¬m trong database (kiáº¿n thá»©c do admin quáº£n lÃ½)
    - Náº¿u khÃ´ng tÃ¬m tháº¥y, tÃ¬m kiáº¿m trÃªn Wikipedia/web
    - DÃ¹ng keyword matching nÃ¢ng cao Ä‘á»ƒ tÃ¬m document vÃ  Ä‘oáº¡n phÃ¹ há»£p nháº¥t
    """

    def __init__(self):
        # Dá»¯ liá»‡u KHÃ”NG cÃ²n láº¥y tá»« thÆ° má»¥c knowledge ná»¯a.
        # Admin sáº½ quáº£n lÃ½ trá»±c tiáº¿p trong database (qua SSMS hoáº·c trang admin).
        pass

    def _search_best_paragraphs(
        self, question: str, max_paragraphs: int = 4, min_score: int = 3
    ) -> Tuple[List[str], List[str]]:
        """
        TÃ¬m cÃ¡c Ä‘oáº¡n phÃ¹ há»£p nháº¥t tá»« nhiá»u document khÃ¡c nhau.
        Chá»‰ láº¥y Ä‘oáº¡n cÃ³ Ä‘iá»ƒm >= min_score Ä‘á»ƒ Ä‘áº£m báº£o liÃªn quan thá»±c sá»±.
        """
        query_tokens = tokenize(question)
        if not query_tokens:
            return [], []

        # LÆ°u táº¥t cáº£ cÃ¡c Ä‘oáº¡n kÃ¨m Ä‘iá»ƒm sá»‘ vÃ  tiÃªu Ä‘á» document
        candidates = []

        with get_session() as db:  # type: Session
            docs = db.query(Knowledge).all()
            if not docs:
                return [], []

            for doc in docs:
                paragraphs = [
                    p for p in (doc.content or "").split("\n\n") if p.strip()
                ]
                if not paragraphs:
                    continue
                
                # TÃ­nh Ä‘iá»ƒm cho tá»«ng Ä‘oáº¡n
                for para in paragraphs:
                    score = score_text(query_tokens, para)
                    # Chá»‰ láº¥y Ä‘oáº¡n cÃ³ Ä‘iá»ƒm >= min_score (liÃªn quan thá»±c sá»±)
                    if score >= min_score:
                        candidates.append({
                            "score": score,
                            "title": doc.title,
                            "para": para.strip()
                        })

        if not candidates:
            return [], []

        # Sáº¯p xáº¿p theo Ä‘iá»ƒm giáº£m dáº§n
        candidates.sort(key=lambda x: x["score"], reverse=True)
        
        # TÃ­nh Ä‘iá»ƒm cao nháº¥t Ä‘á»ƒ lÃ m ngÆ°á»¡ng
        max_score = candidates[0]["score"]
        # NgÆ°á»¡ng Ä‘á»™ng: chá»‰ láº¥y Ä‘oáº¡n cÃ³ Ä‘iá»ƒm >= 60% Ä‘iá»ƒm cao nháº¥t (tÄƒng Ä‘á»ƒ cháº·t cháº½ hÆ¡n)
        threshold = max(min_score, int(max_score * 0.6))
        
        # Láº¥y top N Ä‘oáº¡n, nhÆ°ng Ä‘áº£m báº£o cÃ³ Ä‘oáº¡n tá»« nhiá»u document khÃ¡c nhau
        selected_paragraphs = []
        selected_titles = []
        seen_paras = set()
        doc_count = {}  # Äáº¿m sá»‘ Ä‘oáº¡n tá»« má»—i document
        
        for cand in candidates:
            para_text = cand["para"]
            title = cand["title"]
            score = cand["score"]
            
            # Bá» qua náº¿u Ä‘Ã£ cÃ³ Ä‘oáº¡n nÃ y rá»“i
            if para_text in seen_paras:
                continue
            
            # Chá»‰ láº¥y Ä‘oáº¡n cÃ³ Ä‘iá»ƒm >= ngÆ°á»¡ng
            if score < threshold:
                continue
            
            # Giá»›i háº¡n sá»‘ Ä‘oáº¡n tá»« má»—i document (tá»‘i Ä‘a 2 Ä‘oáº¡n/document)
            doc_para_count = doc_count.get(title, 0)
            if doc_para_count >= 2:
                # Chá»‰ láº¥y thÃªm náº¿u Ä‘iá»ƒm ráº¥t cao (>= 80% Ä‘iá»ƒm cao nháº¥t)
                if score < max_score * 0.8:
                    continue
            
            selected_paragraphs.append(para_text)
            selected_titles.append(title)
            seen_paras.add(para_text)
            doc_count[title] = doc_count.get(title, 0) + 1
            
            # Dá»«ng khi Ä‘Ã£ cÃ³ Ä‘á»§ Ä‘oáº¡n
            if len(selected_paragraphs) >= max_paragraphs:
                break

        return selected_titles, selected_paragraphs

    def answer(self, question: str) -> str:
        question = question.strip()
        if not question:
            return "Báº¡n hÃ£y nháº­p má»™t cÃ¢u há»i cá»¥ thá»ƒ hÆ¡n Ä‘á»ƒ mÃ¬nh cÃ³ thá»ƒ há»— trá»£ nhÃ©."

        # Láº¥y tá»« khÃ³a chÃ­nh tá»« cÃ¢u há»i (bá» stopwords)
        query_tokens = tokenize(question)
        stopwords = {"lÃ ", "vÃ ", "cÃ¡c", "má»™t", "nhá»¯ng", "khi", "Ä‘á»ƒ", "trong", "vá»›i", "thÃ¬", "cÃ³", "khÃ´ng", "gÃ¬", "khÃ´ng"}
        main_keywords = [t for t in query_tokens if t not in stopwords and len(t) > 2]
        
        # BÆ°á»›c 1: TÃ¬m trong database trÆ°á»›c (tÄƒng ngÆ°á»¡ng Ä‘iá»ƒm Ä‘á»ƒ chá»‰ láº¥y Ä‘oáº¡n thá»±c sá»± liÃªn quan)
        titles, paragraphs = self._search_best_paragraphs(question, max_paragraphs=4, min_score=3)

        # BÆ°á»›c 2: Kiá»ƒm tra xem Ä‘oáº¡n cÃ³ thá»±c sá»± liÃªn quan khÃ´ng (pháº£i chá»©a Ã­t nháº¥t 1 tá»« khÃ³a chÃ­nh)
        if paragraphs and main_keywords:
            relevant_paragraphs = []
            question_lower = question.lower()
            seen_paras = set()
            
            for para in paragraphs:
                para_stripped = para.strip()
                if not para_stripped or para_stripped in seen_paras:
                    continue
                
                para_lower = para_stripped.lower()
                
                # Kiá»ƒm tra xem Ä‘oáº¡n cÃ³ chá»©a tá»« khÃ³a chÃ­nh khÃ´ng
                has_main_keyword = any(kw in para_lower for kw in main_keywords)
                
                # Náº¿u khÃ´ng cÃ³ tá»« khÃ³a chÃ­nh, bá» qua (trá»« khi Ä‘iá»ƒm ráº¥t cao)
                if not has_main_keyword:
                    continue
                
                # Bá» qua Ä‘oáº¡n Ä‘áº§u náº¿u nÃ³ chá»‰ lÃ  tiÃªu Ä‘á» giá»‘ng vá»›i cÃ¢u há»i
                if len(relevant_paragraphs) == 0 and len(para_stripped) < 50:
                    if "?" in para_stripped or question_lower in para_lower or para_lower in question_lower:
                        if len(paragraphs) > 1:
                            continue
                
                relevant_paragraphs.append(para_stripped)
                seen_paras.add(para_stripped)
            
            if relevant_paragraphs:
                content = "\n\n".join(relevant_paragraphs)
                return content
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y trong database
        return (
            "Xin lá»—i, mÃ¬nh chÆ°a tÃ¬m Ä‘Æ°á»£c thÃ´ng tin phÃ¹ há»£p trong kiáº¿n thá»©c hiá»‡n cÃ³.\n"
            "Báº¡n cÃ³ thá»ƒ thá»­ há»i láº¡i chi tiáº¿t hÆ¡n, vÃ­ dá»¥: "
            "â€œHTML lÃ  gÃ¬?â€, â€œCÃº phÃ¡p SELECT trong SQL nhÆ° tháº¿ nÃ o?â€, "
            "hoáº·c â€œSá»± khÃ¡c nhau giá»¯a let vÃ  var trong JavaScript?â€.\n\n"
            "ğŸ’¡ Náº¿u báº¡n muá»‘n bot tráº£ lá»i cÃ¢u há»i nÃ y, hÃ£y thÃªm thÃ´ng tin vÃ o database qua trang admin nhÃ©."
        )
